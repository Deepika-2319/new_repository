list of pentaho data integraion steps


Input Steps:

Table Input: Reads data from a database table or query.
Text File Input: Reads data from a delimited or fixed-width text file.
Excel Input: Reads data from Excel files (XLS or XLSX).
JSON Input: Extracts data from JSON formatted files or fields.
XML Input: Reads data from XML files or fields.

Output Steps:

Table Output: Writes data to a database table.
Text File Output: Writes data to a delimited or fixed-width text file.
Excel Output: Writes data to Excel files (XLS or XLSX).
JSON Output: Writes data to JSON formatted files or fields.
XML Output: Writes data to XML files or fields.

Transform Steps:

Select Values: Allows you to select and rename specific fields from the input data.
Filter Rows: Filters data based on specified conditions.
Calculator: Performs calculations on data using various mathematical and logical operations.
Sort Rows: Sorts data based on specified fields and sort orders.
Merge Join: Combines two streams of data based on specified keys.
Split Fields: Splits a single field into multiple fields based on delimiters.
Replace in String: Replaces occurrences of a substring within a field.
Regex Evaluation: Extracts data based on regular expressions.

Database Steps:

Table Output: Writes data to a database table.
Table Input: Reads data from a database table or query.
Database Join: Performs a database join operation on specified tables.
Execute SQL Script: Executes custom SQL scripts.

Flow Control Steps:

Start: Defines the beginning of the transformation.
Dummy: Placeholder step for documentation or flow control purposes.
Success: Defines the end of the transformation in case of successful execution.
Abort: Stops the transformation and marks it as failed.

Utility Steps:

Set Variables: Sets and modifies variables used throughout the transformation.
Get Variables: Retrieves the values of variables for further processing.
Inject Variables: Injects values into the transformation at runtime.

Miscellaneous Steps:

Excel Input/output (legacy): Older Excel Input and Output steps.
REST Client: Performs REST API requests and handles the responses.
User Defined Java Class: Allows custom Java code to be executed within the transformation.

Lookup Steps:

Lookup: Performs a lookup in a reference dataset based on specified keys and retrieves additional information to enrich the main data stream.

Join Steps:

Merge Rows (Diff): Compares two streams of data and identifies differences (rows present in one but not in the other).
Merge Rows (Compare): Compares two streams of data based on specified keys and identifies differences.
Merge Rows (Diff): Compares two streams of data based on specified keys and identifies differences.

Aggregation Steps:

Group by: Groups data based on specified fields and performs aggregate functions (e.g., sum, average, count) on grouped data.

Job Steps:

Job Executor: Allows you to execute another PDI job within the current transformation.

Scripting Steps:

JavaScript: Executes JavaScript code for custom data manipulation and calculations.

Data Validation Steps:

Data Validator: Validates data against defined constraints and rules to ensure data quality.

Data Profiling Steps:

Data Grid: Displays data in a grid format for profiling purposes, allowing you to explore the data.

Data Merging Steps:

Concat Fields: Concatenates multiple fields into a single field.
Merge Rows: Merges multiple input streams into one stream.

Data Cleaning Steps:

String Operations: Performs various string operations, such as trimming, padding, and case conversion.
Value Mapper: Replaces input field values with user-defined values.

Data Validation Steps:

Unique Rows: Removes duplicate rows from the data stream.
Checksum: Calculates checksum values for data integrity validation.

Streaming Steps:

Stream Lookup: Performs lookups on a data stream, enabling real-time processing.

Job Control Steps:

Job Entry: Executes a PDI job from within another PDI job.

Error Handling Steps:

Dummy (error): Acts as a placeholder for handling errors within the transformation.
Error Handling: Catches errors and redirects the data flow based on specified conditions.

Big Data Steps:

Hadoop File Input: Reads data from Hadoop Distributed File System (HDFS).
Hadoop File Output: Writes data to HDFS.

Metadata Steps:

Metadata Injection: Dynamically injects metadata into the transformation at runtime.

File Management Steps:

Delete Files: Deletes files from the specified directory based on certain criteria.
File Exists: Checks whether a file exists in a given directory.

Data Quality Steps:

Data Quality Grouping: Groups similar data values to create more consistent and accurate datasets.
Data Quality Profiling: Analyzes data to generate data quality statistics and metrics.

Data Masking Steps:

Data Masking: Anonymizes sensitive data in the input stream to protect privacy during testing or sharing.

Job Scheduling Steps:

Start Scheduler: Schedules the execution of a PDI job.
Simple Scheduler: Executes a PDI job at specified time intervals.

Decision Steps:

Switch/Case: Routes data based on multiple conditions.
If/Else: Conditional branching based on a specific condition.

Mail Steps:

Send Mail: Sends email notifications based on predefined criteria.

Data Encryption Steps:

Encrypt/Decrypt Files: Performs encryption or decryption of files.

Database Bulk Load Steps:

Bulk Load: Efficiently loads data into a database using bulk insert methods.

Slowly Changing Dimension Steps:

Dimension Lookup/Update: Maintains slowly changing dimensions in data warehousing scenarios.

Data Mining Steps:

Weka Scoring: Uses Weka models to predict outcomes or score data.

Salesforce Steps:

Salesforce Input: Reads data from Salesforce objects.
Salesforce Output: Writes data to Salesforce objects.

Azure Steps:

Azure Blob Storage Input: Reads data from Azure Blob Storage.
Azure Blob Storage Output: Writes data to Azure Blob Storage.

Google Cloud Steps:

Google Cloud Storage Input: Reads data from Google Cloud Storage.
Google Cloud Storage Output: Writes data to Google Cloud Storage.


-------------------------------------------------------------------
explain how it work in real-time


Input Steps:

Table Input: Connects to a database table or executes a query to fetch data. This step is commonly used to read data from relational databases.
Text File Input: Reads data from a delimited or fixed-width text file. It extracts the data from files in a structured manner for further processing.
Excel Input: Reads data from Excel files. It allows you to extract data from spreadsheets for analysis and transformation.

Output Steps:

Table Output: Writes data to a database table. This step is often used to load transformed data back into the database or update existing records.
Text File Output: Writes data to a delimited or fixed-width text file. It's commonly used to save processed data into human-readable files.
Excel Output: Writes data to Excel files. It allows you to generate Excel reports or update existing sheets with processed data.

Transform Steps:

Select Values: Selects specific fields from the input data and optionally renames them. It helps in reducing the data to only relevant columns.
Filter Rows: Filters data based on specified conditions. For instance, you can filter data to include only certain date ranges or specific categories.
Calculator: Performs calculations on data using various mathematical and logical operations. You can create new derived fields based on existing data.

Flow Control Steps:

Start: Defines the beginning of the transformation, and data processing flows from this step.
Success: Defines the end of the transformation when it completes successfully, and the data flows through this step.
Abort: Stops the transformation and marks it as failed in case of errors or specific conditions.

Database Steps:

Table Output: Writes data to a database table. It is used to insert, update, or delete records in the database.
Table Input: Reads data from a database table or query. It is used to fetch data from the database for further processing.

Scripting Steps:

JavaScript: Executes custom JavaScript code within the transformation. It enables you to perform complex data manipulations using scripting.

Data Validation Steps:

Data Validator: Validates data against defined constraints and rules to ensure data quality. For example, you can validate if certain fields meet specific conditions.

Data Profiling Steps:

Data Grid: Displays data in a grid format for profiling purposes, allowing you to explore the data and understand its characteristics.

Data Masking Steps:

Data Masking: Anonymizes sensitive data in the input stream to protect privacy during testing or sharing. This step is crucial for ensuring data security and compliance with data protection regulations.

Job Scheduling Steps:

Start Scheduler: Schedules the execution of a PDI job at specific intervals or time frames. It allows you to automate the execution of data integration tasks.
Decision Steps:

Switch/Case: Routes data based on multiple conditions. It enables conditional branching and handling different scenarios based on the data.
Mail Steps:

Send Mail: Sends email notifications based on predefined criteria, such as success or failure of the transformation. It's helpful for monitoring and reporting purposes.

Data Encryption Steps:

Encrypt/Decrypt Files: Performs encryption or decryption of files. This step is essential for securing sensitive data during file transfers or storage.

Database Bulk Load Steps:

Bulk Load: Efficiently loads data into a database using bulk insert methods. This step optimizes the loading process for large datasets, improving performance.
Slowly Changing Dimension Steps:

Dimension Lookup/Update: Maintains slowly changing dimensions in data warehousing scenarios. It handles historical changes to dimension attributes.

Data Mining Steps:

Weka Scoring: Uses Weka models to predict outcomes or score data. This step is valuable for predictive analytics and machine learning applications.

Salesforce Steps:

Salesforce Input: Reads data from Salesforce objects. It allows integration with Salesforce data for further processing.
Salesforce Output: Writes data to Salesforce objects. It enables updating or inserting data into Salesforce.

Azure Steps:

Azure Blob Storage Input: Reads data from Azure Blob Storage. It allows data integration with Azure-based storage solutions.
Azure Blob Storage Output: Writes data to Azure Blob Storage. It facilitates storing data in Azure Blob Storage.

Google Cloud Steps:

Google Cloud Storage Input: Reads data from Google Cloud Storage. It enables integration with Google Cloud storage solutions.
Google Cloud Storage Output: Writes data to Google Cloud Storage. It allows storing data in Google Cloud storage.

Data Validation Steps:

Unique Rows: Removes duplicate rows from the data stream. It ensures data uniqueness based on specified fields.
Checksum: Calculates checksum values for data integrity validation. It helps in detecting data corruption or changes during data movement.

Streaming Steps:

Stream Lookup: Performs lookups on a data stream, enabling real-time processing. It is useful for enriching streaming data with reference information.


